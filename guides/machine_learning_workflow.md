# A typical Machine Learning Workflow
It is no secret that understanding data is the key to unlocking hidden potential across our Public Sector organisations. Be it through optimisations in analysis, the monitoring of services or identification of previously unknown trends, the importance of data is unquestionable. So what part do Made Tech play in this brave new data driven world? Well, as ever, our role is one of guidance, education and reliable implementation. We must ensure that we posses the ability to mentor this burgeoning capability within the public sector and the confidence to lead them through the data landscape. 

The goal of this document is to outline the framework for a typical Machine Learning (ML) project. We will briefly discuss each step and its importance, what factors must be considered and what realities our clients (and our data teams) are likely to face. We will cover how to identify business needs and how to frame problems as questions that can be answered by a machine learning model. We will explore the realities of developing a high quality data set and what is necessary to develop a robust ML solution. Core to this process, is how it will impact the business and how best we can demonstrate the value added to the client.

![A typical machine learning workflow](../images/ml-workflow.jpg)

Software development is always an iterative process, agile workflows and short sprint cycles are now well known and widely accepted as best practices within the public sector. Although this workflow is mirrored in the data world the nature of ML development can often make it seem like very little is ever being done. The process of model development is incredibly iterative and will produce very little in terms of tangible outputs until it is complete. The cycle of feature engineering and model training is in many ways far more similar to a tech spike, trying new techniques and models until something yields the desired result. Where a typical sprint may add a selection of new features to an app, a successful ML sprint may only see several new columns (data features) added to a table. It may take several weeks to prepare the data and develop features, only to find that the resulting model does not reach the required metrics and the whole process must begin once again.

The implementation of ML is difficult, it takes a lot of time and money and there will always be the risk that a project may run out of either of these before a successful model can be produced. This makes open and honest communication incredibly important with any stakeholder. 

## Identifying and Framing the Business Problem 
The first objective of any speculative data project is convincing stakeholders of the need for a data strategy. Highlighting the myriad of benefits that accompany a mature data platform is an incredibly exciting process, as usually the development of a modern data pipeline alone will unlock possibilities and save the client time and money. Made Tech already has a track record of this and plenty of case studies to back it up. Once a conversation around data has begun, things can quickly get quite exciting. 

Machine Learning has an incredible range of problems it can solve, processes it can automate and insights it can generate. During these early conversations, it is a good idea to start talking to a member of the data team. You will quickly find that we are an excitable bunch who love talking about the art of the possibility and will very quickly have an idea of the potential the client can unlock. For now, let's begin with this; broadly speaking ML tasks are likely to fall into three basic categories:

**Forecasting**: Using historical data to predict future outcomes. How many passengers are likely to use public transport at this time or over that weekend? Given previous trends, how many patients should a hospital expect, when are the likely surges and what will demand be? Predicting staff requirements for shift allocation, estimating supply demands or forecasting spending patterns. These are all questions that can be tackled using regression modelling.

**Classification or recognition**: Identifying specific events based on input data. Using previous trends, can we predict which workers are likely to leave and why? Can we detect when an application to a service is fraudulent? Do we need to automatically track how many of a specific request type are submitted? Does the client need to filter spam from messaging services? Classification problems can even extend to include image recognition, automatically detecting contraband items in X-Ray images, checking for false identification documents at border crossings or extracting license plate information from CCTV footage. Whenever you are looking to apply a specific label, be that to text, images or users, chances are you can train a classification model to do the work for you.

**Clustering**: Finally there is clustering. This is a version of unsupervised learning that can be used to determine or specify user groups based on selected metrics. Highlight similarities between teams, departments, organisations and locations, or even find outliers in a data set that might otherwise go unnoticed.

There are undeniably countless instances where the implementation of an ML model can deliver massive benefits to a department. The goal is to identify those opportunities and communicate to the client the huge benefits of doing so. Once the conversation has begun, a first step could even be bringing a data scientist on board to look at the data and begin building out a proof of concept. Unlocks in efficiency and improvements in reporting capabilities are massive wins for any department so it can very quickly become an exciting opportunity for a stakeholder. Even if the data scientist isn't able to produce anything, they are likely to highlight gaps in infrastructure or flaws with their current data strategy, which in itself opens up entirely new conversations and possible work. It all begins with a conversation. 

## Data Collection and Preparation
It may come as no surprise that the core of any ML project is the data, given this is what is used to train a model and evaluate its effectiveness. The overall success of any machine learning project relies entirely on there being enough high-quality, representative data available to analyse. Only from this can we hope to draw meaningful conclusions. As a rule of thumb, the bigger the raw data set the better. As the team proceed through numerous development and model training iterations, the aim will be to produce a core of very high-quality data that will be segmented into training, validation and testing sets and fed to the model. Without a large, initial data set, there is the risk that this process will reduce the data set to the point that the data is no longer representative of the business problem resulting in a poorly trained model that is unreliable. 

Considerations must always be made around the data collection process. When gathering potentially gigabytes of data it is vital to ensure that it is correctly structured, schemas agreed upon, relationships drawn and documentation written. Thought must be given to whether the data source is reliable and if it truly represents whatever situations you wish to analyse. Is the data robust enough to account for changes in trends or unexpected events? If you are looking to perform a time series analysis, does the data cover a long enough period of time? If you are looking to perform image recognition, do you have a large enough dataset of correctly labelled images? Data collection is a time-intensive and resource-heavy process and should not be rushed. A model can only ever be as good as the data it is trained on and there are very real moral and ethical implications to releasing a poorly trained model. 

Once the data has been collected then we must consider where it is stored and how easily it can be accessed. Who has permission to view the data and what infrastructure is required to support it? Government data is often considered, at minimum, Official Sensitive, what processes are in place to ensure its security? Is there any personal data stored in the raw data? Does it need to be anonymised? Data at the scale required for effective machine learning can rarely be stored in simple excel sheets, therefore building out a robust data infrastructure is key. It is also a fantastic development for any business, and of course, one that Made Tech has plenty of experience in delivering. The goal here is to move from single excel sheets held on an individual's computer, to managed databases hosted on a suitable cloud platform that can be monitored, updated and maintained. 

During these early stages, a data scientist can be brought in to consult on the data collection process and build out a proof of concept in a notebook to present to the client. This can usually be done with an excel extract and some time with the data. It must be stressed however that this is not a solution to a problem, it is part of the sales pitch. In order to build a robust ML solution, the client will likely need to modernise their data storage solution. The building of this solution can be handed to data engineers (the unsung heroes of any successful ML project) while the data scientists either continue to familiarise themselves with the data or step back from the project to allow this first phase to be complete. 

## Exploratory Data Analysis and Visualisation
Once the data has been collected and made accessible, it is time to begin the process of assessing its quality. Any modelling exercise requires as much very high-quality data as possible. The team must assess whether they have gathered enough raw data from which they can distil a gold standard training set, or whether the collection process needs to continue. This initial interrogation is referred to as exploratory data analysis (EDA). 

Through this process, the data scientists will continue to cultivate a deeper understanding of the subject matter, now seen through the ever-sharpening lens of quantitative data. They will uncover where data is missing, through poor reporting or collection, how various features interact with one another and will begin to build up a picture of how the data may fit into a model. They may even be able to begin to have an idea as to which models may suit the problem best. If they have been heavily involved in the data collection process then there is the potential that this will be a fairly quick process. If however, they are coming to a data cold then, depending on the size of the data set, it could take several weeks for them to fully grasp the shape and quality of the data.  

Throughout this process, a vital tool in the data scientist's arsenal is data visualisation. This serves two purposes. Firstly, as an excellent tool for intuitive analysis. It is far easier to identify relationships and correlations from a plot than it is from a column of numbers that will likely be several tens if not hundreds of thousands of rows long. The second role of visualisations, easily overlooked by the eager data scientist lost in the data, is one of communication. EDA does not often produce outputs in the same way as regular development, presenting meaningful plots and graphs to the client reassures them that work is being done and progress is being made. It will also likely be a spark for conversation as stakeholders see, often for the first time, data that supports (or contradicts) their hard-earned industry expertise. Encouraging this conversation is a great way to drive a project forward and build momentum and backing. With the right thought, even this early analysis can be seen as a massive win for the project and its stakeholders and provides tangible results that can be paraded to the wider business. 

## Feature Engineering, Model Training and Model Evaluation
Feature engineering is a fundamental component of any machine learning project and where a data scientist really earns their salt. This is the process of selecting, manipulating, extracting and otherwise transforming raw data into "features" that can be fed into a model and used to make predictions. When done correctly, feature engineering will reduce a model's complexity while retaining the detail necessary to deliver accurate predictions. Although feature selection is core to the machine learning workflow, there remains no set process or hard rules. It is for this reason that among data practitioners feature engineering is considered as much an art form as it is a scientific process. 

Hand in hand with this highly iterative process is model selection and evaluation. Every machine learning problem has a plethora of different algorithms that may be used to solve them. It may be that the nature of the data and the trends it contains respond better or worse to certain algorithmic approaches. The nature of ML means it is often almost impossible to know for certain until it is tried. Each attempted model may require a slightly different set of features, there are then hyper-parameters for each model that must also be tuned. With each attempt the resulting models must be evaluated against mathematical and business metrics before a final selection can be made. 

All of this leads to a very time-consuming process, often requiring many iterations before a final feature set and model selection is made. It may even transpire that the situation is not completely described by the current data set and it is necessary to return to the data collection process before the cycle may begin again. It is vital that throughout this process the client is kept well informed of what is going on. Sometimes the result of a feature engineering sprint will simply be reporting what doesn't work, important information that translates to progress, but something that can be frustrating for a client looking for tangible deliveries. 

## Business Goal Evaluation
At all times throughout the data collection, feature engineering and model training process it is vital to maintain sight of the business goal. The implementation of any ML solution has the potential to have a massive impact on the operation of any department, freeing up resources, delivering deeper insight and taking months off delivery time scales. It is also very easy for a data project to lose sight of its initial goal and suck up valuable time and money from a department. Knowing when to halt a project and reevaluate the business requirements is extremely important. ML projects are expensive endeavours, often it is the job of the data scientist to stand up in a meeting and say stop. 

Another key consideration, particularly when considering the sector in which Made Tech operates, is the downstream consequences of our solutions. A model can only ever provide an outcome with a certain level of confidence, there will always be outlier cases that the model is not able to predict. In simple terms, it will not be correct 100% of the time. Made Tech must consider the impact of those errors. What happens if the model overestimates a figure for a report, who is seeing that report and what decisions are being made? If that model is classifying fraudulent benefits claims, does the model show bias? What impact will a misclassification have on a person's life? 

These are questions that must be asked and deeply considered when building a service. Made Tech has a responsibility to always deliver the best solution to a problem. Sometimes that may mean developing exciting machine learning pipelines, other times the situation may be better served by taking the time to fully map out the business logic and "simply" building that into the pipeline instead.

## Hosting, Deploying and Predicting. 
Once the client is satisfied with the quality of the model, it is time to deploy it in the production environment. As with every other step so far, there are a variety of things that must be considered ranging from where the model is hosted to how it is monitored to who is responsible for the ongoing maintenance. Every scenario is different and each will have its unique answer. It is typically at this point that again data engineers step to the forefront of development. The engineers will have a far greater understanding of the infrastructure that has been developed to support this solution and hopefully know best how to combine the two. 

Model selection will have an impact on how and where it is stored. Some models are relatively small, comprising of a set of numeric parameters equating to a few kilobytes of information. Others could be many gigabytes as they contain complex vectors for every row of data in the training set. Some models can work in a live environment, generating predictions with single-digit latency, others may require longer processing times as they retrain and analyse the input against the existing data. Is the goal to provide a daily/weekly/monthly report to a head of the department, or is it to provide live feedback to an officer on the front line? Is there another layer of data translation that needs to sit between the model and the user in order to make the output usable?

Once the model is actively producing predictions using live data, that model has to be constantly monitored to ensure that it continues to deliver accurate results. It is said that no plan survives first contact with the enemy, well the same can be said about machine learning models. Data will always evolve with time, whether through new trends organically coming to centre stage or a new data team changing the name of a field in the raw data, changes will occur and will affect the model outputs. It is vital that active monitoring is built into any solution, this can take the shape of an email alert that is triggered when an evaluation metric is breached or a fully featured dashboard that reports them live with every new submission. Either way, the model will need to be supervised and monitored to ensure that it doesn't fall outside acceptable operating levels. 

This means that there needs to be a plan in place around who is going to own that maintenance going forward. There will, of course, be a component of upskilling clients with every Made Tech delivery, does that include enough in-depth training around the various machine learning steps? In the best-case scenario, automated retraining of the model can be built into the delivered data pipeline. Meaning the steps of feature engineering, model training and maybe even hyper-parameter selection can occur with minimal input from the admins. It is far more likely, however, that as trends change, the feature engineering and model selection process will need to be revisited and iterated upon. Future maintenance of the data pipeline needs to be discussed and plans for these eventualities need to be put in place. 

## Closing Words
The world of data is incredibly exciting and expanding every day. Working in the public sector means we have access to an incredible wealth of data that presents deeply interesting challenges and provides the opportunity to deliver real and lasting impact. I hope that this document has played its part in demystifying a typical machine learning workflow. If there are any further questions then please join the #COP-Data slack channel and ask. You will find that the data team love talking about data and are rather easily distracted by an interesting problem! 
